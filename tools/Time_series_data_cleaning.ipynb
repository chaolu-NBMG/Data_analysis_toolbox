{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a62a1f-ea1c-4c2e-96fe-ffde434998c3",
   "metadata": {},
   "source": [
    "# Time Series Data Cleaning  \n",
    "**Author:** Chao Lu  \n",
    "**Last update:** 2024/07/01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09ffb9-df03-4398-aa12-91cdeeb84b16",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1666d10-ff98-4c89-b768-367df147113a",
   "metadata": {},
   "source": [
    "This notebook addresses the challenge of cleaning a time series dataset, which contains irregular entries. The dataset should ideally have data recorded at consistent 5-minute intervals (e.g., 11:45, 11:50, 11:55, 12:00, etc.). However, it has been observed that there are extraneous entries at times like 11:56 or randomly at xx:02, which disrupt the expected 5-minute sequence. This inconsistency in time intervals can lead to issues in subsequent data analysis and forecasting tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8601b7-75b1-4142-a375-9e15282a2a38",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "1. **Identify Extraneous Entries**: Detect and identify data points that do not adhere to the standard 5-minute interval.\n",
    "2. **Maintain 5-minute Spacing**: Ensure that the dataset only includes entries at 5-minute intervals, removing all extraneous entries.\n",
    "3. **Ensure Data Completeness**: After removing the irregular entries, check for any gaps in the 5-minute intervals and address any missing data points to maintain a complete and consistent dataset for accurate analysis.\n",
    "\n",
    "By accomplishing these goals, the dataset will be structured for better usability in time series analysis, ensuring reliability and consistency in data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0392b1-6871-4bdc-99ca-57c68266d252",
   "metadata": {},
   "source": [
    "## Load the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd8f2c-4c38-49f7-9f2e-35c73557e247",
   "metadata": {},
   "source": [
    "The first step is to install and import the necessary packages for this task. Define the file path and file name to load the dataset effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48781aa8-1dd1-4003-857d-e90f2dff0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 12 17:15:28 2024\n",
    "@author: chao lu\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path and filename of the dataset\n",
    "path = r'C:\\Users\\chaolu\\workspace\\time series data'\n",
    "filename = 'weather_data_utf8.csv'\n",
    "target_file = os.path.join(path, filename)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(target_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e505f-2e9e-4add-b57e-0c316ee8ca31",
   "metadata": {},
   "source": [
    "## Identify and Handle Inconsistent Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b33bcd-164d-42e7-b2cd-23c1506faef5",
   "metadata": {},
   "source": [
    "This section focuses on identifying and correcting inconsistencies in timestamp intervals within the dataset. The goal is to ensure that all timestamps align precisely with expected 5-minute intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e2b91-63d3-4c04-9471-d0ab7625821a",
   "metadata": {},
   "source": [
    "**Convert Date/Time to DateTime:** First, the 'Date/Time' column is converted into a datetime format, manually adding the year 2023 for consistency and to facilitate datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0360ab-228e-4df6-9fb1-f90f5827da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'].apply(lambda x: x + ', 2023'), format='%b %d, %I:%M %p, %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af96e6-3d99-454b-9ca6-28998b4711cb",
   "metadata": {},
   "source": [
    "**Round Timestamps to Nearest 5 Minutes:** Each timestamp is rounded to the nearest 5-minute interval to standardize the data entries. This rounded time is then used to identify timestamps that precisely match the expected 5-minute marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405bfe7f-bbfa-4d2a-83da-b8fca0966e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rounded Time'] = df['Date/Time'].dt.round('5min')\n",
    "df_on_interval = df[df['Rounded Time'] == df['Date/Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e4cac-7f18-4708-afed-2bd969ff86ec",
   "metadata": {},
   "source": [
    "**Determine Range of Timestamps:** By finding the earliest and latest timestamps within these rounded times, we establish the range of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5beab7-1912-45dd-9168-0350ead97af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = df_on_interval['Date/Time'].min()\n",
    "end_time = df_on_interval['Date/Time'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9d10e-e684-414b-8d6b-71f0dc5a05db",
   "metadata": {},
   "source": [
    "**Generate Expected Sequence of Timestamps:** A complete sequence of 5-minute intervals is generated between the earliest and latest times identified. This sequence represents the ideal timestamps expected in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a38607-7803-4dff-8d82-d42997a82e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_times = pd.date_range(start=start_time, end=end_time, freq='5min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df79f67-557a-47d3-91cb-384971956c2b",
   "metadata": {},
   "source": [
    "**Filter Out Inconsistent Entries:** The dataset is then filtered to include only those entries whose timestamps match exactly with the generated 5-minute intervals. This step removes any rows that do not conform to the expected timing, such as random entries at non-standard times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79e85e0-a170-4f24-9380-a306c68cbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df['Date/Time'].isin(expected_times)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104fa67-70b8-4878-af64-e164dc6ef8ca",
   "metadata": {},
   "source": [
    "**Report and Remove Inconsistencies:** Calculate the number of rows that had inconsistent timestamps and were removed. This measure gives insight into the extent of the cleaning required and confirms the application of the specified data handling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4820c3-b2d6-4a64-97c5-f66664f7af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inconsistent format rows: 2460\n",
      "Dealing method: Entire row remove\n"
     ]
    }
   ],
   "source": [
    "num_inconsistent = len(df) - len(df_clean)\n",
    "print(f\"Total number of inconsistent format rows: {num_inconsistent}\")\n",
    "print(\"Dealing method: Entire row remove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832bd1b-6800-4be3-bf58-47488b6f1179",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3398a4-0dab-4066-bbd0-c3e9053114b9",
   "metadata": {},
   "source": [
    "This section is dedicated to identifying and removing duplicate entries in the dataset to ensure each timestamp is represented uniquely. Maintaining a singular record for each 5-minute interval is crucial for the accuracy of time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c4c6d-056d-43b6-a453-3aa3bb70ac76",
   "metadata": {},
   "source": [
    "**Sort Data by Date/Time:** First, the data is sorted in ascending order by 'Date/Time'. This step ensures that when duplicates are identified, the selection of which entry to retain (the latest occurrence) is accurately applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3248b7-755c-40a9-8a70-86fd4a624d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.sort_values(by='Date/Time', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea0a92-e65b-4c44-901d-f18db7b7c7cc",
   "metadata": {},
   "source": [
    "**Identify Duplicates:** Before removing duplicates, it's important to first identify them. This is done using the duplicated method, which flags all entries that have a duplicate 'Date/Time' stamp, allowing for an accurate count of duplicates before any are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6634def-e448-40c4-8b58-43024b80d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_before = df_clean.duplicated(subset=['Date/Time'], keep=False)\n",
    "num_duplicates = duplicates_before.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cacace-5f0d-4f02-81b4-b4be6894b5b0",
   "metadata": {},
   "source": [
    "**Remove Duplicate Entries:** Duplicates are removed, keeping only the last occurrence of each timestamp. This approach is chosen because the latest record is often the most accurate or complete in many data collection processes, ensuring that the best possible data is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f486b8ee-fefb-4d4a-9608-035e591cd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop_duplicates(subset=['Date/Time'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3b39d-2d52-492f-8e5c-438259a3356b",
   "metadata": {},
   "source": [
    "**Report Removal of Duplicates:** After the duplicates are removed, the number of duplicates that were found and handled is reported. This transparency helps in understanding the impact of this step on the dataset's integrity and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddb66be-67e3-4417-b571-0e6fa120d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate rows: 24\n",
      "Dealing method: Retain the latest record for duplicates\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of duplicate rows: {num_duplicates}\")\n",
    "print(\"Dealing method: Retain the latest record for duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650646e5-53bb-4843-8c3d-d512884ed143",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58299619-c23a-4012-bbf3-2d369ac110ef",
   "metadata": {},
   "source": [
    "In this section, we will check the data types and may change them based on the nature of the data. For example, the 'Wind_Speed_kph' should not be of type 'object'. We will inspect the data and convert it to float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1020551-a99a-45be-9bae-e1bd2839de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26313 entries, 0 to 28783\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   Date/Time               26313 non-null  datetime64[ns]\n",
      " 1   Temp_C                  26280 non-null  float64       \n",
      " 2   Dew_Point_C             26280 non-null  float64       \n",
      " 3   RH                      26280 non-null  float64       \n",
      " 4   Heat_Index_C            99 non-null     float64       \n",
      " 5   Wind_Chill_C            6186 non-null   float64       \n",
      " 6   Wind_Dir                26205 non-null  object        \n",
      " 7   Wind_Speed_kph          26313 non-null  object        \n",
      " 8   Visibility_km           26278 non-null  float64       \n",
      " 9   Weather                 1445 non-null   object        \n",
      " 10  Clouds_x100_ft          26304 non-null  object        \n",
      " 11  Sea_Level_Pressure _mB  0 non-null      float64       \n",
      " 12  Station_Pressure_mB     26313 non-null  float64       \n",
      " 13  Altimeter_Setting_mB    26313 non-null  float64       \n",
      " 14  Pre_1Hour Precip_mm     723 non-null    float64       \n",
      " 15  3_Hour_Precip_mm        0 non-null      float64       \n",
      " 16  6_Hour_Precip_mm        0 non-null      float64       \n",
      " 17  24_Hour_Precip_mm       0 non-null      float64       \n",
      " 18  6_Hr_Max_C              0 non-null      float64       \n",
      " 19  6_Hr_Min_C              0 non-null      float64       \n",
      " 20  24_Hr_Max_C             0 non-null      float64       \n",
      " 21  24_Hr_Min_C             0 non-null      float64       \n",
      " 22  Rounded Time            26313 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(17), object(4)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa54b7-f484-49a9-ba45-151b1ffe727d",
   "metadata": {},
   "source": [
    "To convert the `df_clean` 'Wind_Speed_kph' column to float numbers, we need to handle the entries that contain the 'G' notation, which likely represents gusts of wind. We will remove any characters after 'G' and retain only the numerical part before 'G'. After cleaning the data to ensure it contains only numerical values, we will convert the entire column to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fe2cbe-f90f-41f8-b19a-c1dd169e80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the gust notation and convert to float\n",
    "df_clean['Wind_Speed_kph'] = df_clean['Wind_Speed_kph'].str.extract(r'(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726744c-1ac3-47f7-b11d-ff33e0aa7c28",
   "metadata": {},
   "source": [
    "## Handle Missing Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3621f-c07b-4a70-a42d-b6cc77cc0c99",
   "metadata": {},
   "source": [
    "In this section, we ensure that the dataset has a complete series of records for every expected 5-minute interval by identifying and addressing any missing timestamps. This step is crucial to prevent any gaps that could affect time series analysis and forecasting accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe384a7-e9f2-4245-8b52-4f4f19cdd6ff",
   "metadata": {},
   "source": [
    "**Generate a Complete Time Range DataFrame:** First, a DataFrame is created that lists all expected timestamps based on the previously established 5-minute interval range. This DataFrame serves as a template to ensure no gaps exist in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49f9a1d1-8c99-491c-b88b-51ce568b2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times = pd.DataFrame(expected_times, columns=['Date/Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e57af-136c-4cbf-be45-7a3dd47d0af4",
   "metadata": {},
   "source": [
    "**Merge with Cleaned Data:** The DataFrame containing all expected timestamps is merged with the cleaned data. This merge is performed as a left join, which ensures that all timestamps in all_times are included in the resulting DataFrame, df_final, with data from df_clean aligned where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3408da2a-60b3-4661-8b85-ccbedf4c5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(all_times, df_clean, on='Date/Time', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6f61c-af1f-4eae-9758-c410c500704b",
   "metadata": {},
   "source": [
    "**Identify Missing Timestamps:** By comparing the complete time range with the timestamps present in the cleaned data, we can identify any missing timestamps. This step is essential for quantifying the extent of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4adfa047-ce44-4243-9979-4d4dc30cdb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing timestamps: 183\n"
     ]
    }
   ],
   "source": [
    "missing_timestamps = all_times[~all_times['Date/Time'].isin(df_clean['Date/Time'])]\n",
    "num_missing_timestamps = len(missing_timestamps)\n",
    "print(f\"Number of missing timestamps: {num_missing_timestamps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519df896-4e2e-4781-8c9d-3fbc45276c3e",
   "metadata": {},
   "source": [
    "**Apply Filling Strategies:** For the columns identified as critical for analysis (e.g., temperature, dew point, and relative humidity), missing values are interpolated linearly from the nearest existing data points. This method of interpolation provides a reasonable approximation for missing data, ensuring continuity in the dataset's environmental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b03bc3f3-5690-4953-88a8-c7345edc6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = ['Temp_C', 'Dew_Point_C', 'RH', 'Wind_Speed_kph', 'Station_Pressure_mB', 'Altimeter_Setting_mB']  # Example columns\n",
    "for column in df_final.columns:\n",
    "    if column in columns_to_fill:\n",
    "        df_final[column] = df_final[column].interpolate(method='linear', limit_direction='both', limit_area='inside')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ba8ec-c56b-43fe-befc-c51dbd784ff0",
   "metadata": {},
   "source": [
    "**Leave Non-Essential Columns with NaN:** Columns not specified for interpolation are left with NaN where data is missing. This decision is made to avoid introducing potentially misleading data into the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b658a1-4be1-4708-b6a9-8f2893a7789f",
   "metadata": {},
   "source": [
    "**Report on Data Handling Method:** Finally, the chosen method for handling missing data and the columns affected are clearly documented. This transparency helps in understanding how data integrity is maintained and ensures reproducibility of the dataset preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25886205-2774-4c00-adb7-8599b2a3ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing method: Apply linear interpolation to fill missing values in specified columns: Temp_C, Dew_Point_C, RH, Wind_Speed_kph, Station_Pressure_mB, Altimeter_Setting_mB\n"
     ]
    }
   ],
   "source": [
    "print(\"Dealing method: Apply linear interpolation to fill missing values in specified columns: \" + ', '.join(columns_to_fill))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949118c4-ddb0-4a10-b1ad-3f377284b575",
   "metadata": {},
   "source": [
    "## Save the Final Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30d76e-3635-46c4-a06e-8fe39241bf63",
   "metadata": {},
   "source": [
    "This section outlines the final steps to save the thoroughly cleaned and processed dataset, ensuring that the data is ready for further analysis or sharing. This is a critical step in preserving the integrity of the cleaned dataset and ensuring its usability in future applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c56432-bf1c-4608-836f-8013f269800b",
   "metadata": {},
   "source": [
    "**Remove Unnecessary Columns:** First, any columns added during the data cleaning process that are no longer needed for analysis are removed. For this dataset, the 'Rounded Time' column, which was used to align and check the timestamps, is removed to streamline the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e5ec11-cdf1-4536-9d7e-8cd75e6929ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['Rounded Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed782a-6d90-4353-9e93-9b1ac6e9b4d2",
   "metadata": {},
   "source": [
    "**Specify the Output File Path:** Define the path where the cleaned dataset will be saved. This path includes the directory and the filename. The filename is chosen to reflect that the dataset is the final version after all cleaning steps have been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d90d8d4-30a4-4399-ac71-982dc06515fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = os.path.join(path, 'df_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb398a-67d4-4518-9dc0-652aaa6b621e",
   "metadata": {},
   "source": [
    "**Save the Dataset to a CSV File:** The cleaned dataset is saved to a CSV file. The index=False parameter is used to prevent pandas from writing row numbers (index) into the CSV file, which keeps the file clean of any unnecessary data. The encoding 'ISO-8859-1' is specified to ensure compatibility with various systems and software that might read this CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50b07840-636f-4891-90c7-822e82e6d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(output_file_path, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48c97c-ad4c-4965-9f30-e4e89d665308",
   "metadata": {},
   "source": [
    "**Confirm Successful Save:** Finally, a confirmation message is printed to indicate that the data has been successfully saved to the specified location. This step provides immediate feedback that the save operation has been completed without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4084f5e1-a0f6-4b99-8825-c8d1c115dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned time series data saved to: C:\\Users\\chaolu\\workspace\\time series data\\df_final.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned time series data saved to:\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
